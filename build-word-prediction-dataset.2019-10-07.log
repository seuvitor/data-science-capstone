
R version 3.6.0 (2019-04-26) -- "Planting of a Tree"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(textclean)
> library(dplyr)

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union

> library(quanteda)
Registered S3 methods overwritten by 'ggplot2':
  method         from 
  [.quosures     rlang
  c.quosures     rlang
  print.quosures rlang
Package version: 1.4.3
Parallel computing: 2 of 4 threads used.
See https://quanteda.io for tutorials and examples.

Attaching package: ‘quanteda’

The following object is masked from ‘package:utils’:

    View

> 
> # Function to download the dataset if it is not present on the directory.
> downloadDataset <- function() {
+     datasetFileName <- "Coursera-SwiftKey.zip"
+     
+     datasetUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
+     
+     if (!file.exists(datasetFileName)) {
+         download.file(datasetUrl, datasetFileName)
+     }
+     
+     if (!file.exists("final")) {
+         unzip(datasetFileName)
+     }
+ }
> 
> # Function to use when sampling lines from a text file. It uses a closure to
> # remember what lines were already sampled and should be skipped when sampling
> # again without replacement.
> skippedLines <- function() {
+     skipLine <- logical(2500000)
+     lineNum <- 0
+     resetLineCount <- function() {
+         lineNum <<- 0
+     }    
+     nextLine <- function() {
+         lineNum <<- lineNum + 1
+     }
+     setSkip <- function(skip) {
+         skipLine[lineNum] <<- skip
+     }
+     getSkip <- function() {
+         skipLine[lineNum]
+     }
+     getLineNum <- function() {
+         lineNum
+     }
+     getSkipLine <- function() {
+         skipLine
+     }
+     list(resetLineCount=resetLineCount,
+          nextLine=nextLine,
+          setSkip=setSkip,
+          getSkip=getSkip,
+          getLineNum=getLineNum,
+          getSkipLine=getSkipLine)
+ }
> 
> # Function to sample from a text file of the SwiftKey raw text dataset.
> loadCorpusSample <- function(language, category, samplingProbability) {
+     skipped$resetLineCount()
+     filePath <- file.path("final",
+                           language,
+                           paste(language, category, "txt", sep = "."))
+     
+     sampledLines <- character()
+     con <- file(filePath, "r")
+     while (TRUE) {
+         line <- readLines(con, 1, skipNul = TRUE)
+         if (length(line) == 0) {
+             break
+         }
+         skipped$nextLine()
+         if (!skipped$getSkip() && rbinom(1, 1, samplingProbability)) {
+             sampledLines <- c(sampledLines, line)
+             skipped$setSkip(TRUE)
+         }
+     }
+     close(con)
+     sampledLines
+ }
> 
> # Very basic text normalization to apply to the raw data.
> normalizeText <- function (lines) {
+     lines <- sapply(lines, replace_non_ascii)
+     lines <- sapply(lines, strip)
+     lines
+ }
> 
> # Gets a list of words to use as reference when discarding profanity from text.
> getBadWords <- function() {
+     badWordsFileName <- "badwords-en.txt"
+     badWordsUrl <- "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
+     if (!file.exists(badWordsFileName)) {
+         download.file(badWordsUrl, badWordsFileName)
+     }
+     readLines(badWordsFileName)
+ }
> 
> # Keeps the lines of text that do not contain profanity.
> discardLinesWithProfanity <- function (lines) {
+     badWords <- getBadWords()
+     lines[!grepl(paste("\\b(", paste(badWords, collapse = "|"), ")\\b"),
+                  lines,
+                  ignore.case = TRUE)]
+ }
> 
> # Discard lines of text that appear to be automatically generated by bots on
> # twitter.
> discardLinesWithAutomaticMessage <- function (lines) {
+     automatic <- c("thanks for the follow",
+                    "thank you for the follow",
+                    "thanks for the ff",
+                    "thanks for the rt",
+                    "thank you for the rt",
+                    "thanks for the mention",
+                    "thanks for the shout out")
+     lines[!grepl(paste(automatic, collapse = "|"),
+                  lines,
+                  ignore.case = TRUE)]
+ }
> 
> # Function to return the minimum count of collocations of words, of each size,
> # that should be considered for this dataset. These thresholds were defined
> # after analysing the relative frequencies of collocations of words of size
> # 2 to 5 on a 5% sample of each set of texts (twitter, blogs and news). On these
> # samples, such thresholds were appropriate to keep approximately the top 10%
> # most frequently ocurring collocations.
> getMinCountForNGramSize <- function(size) {
+     if (size == 2) {
+         12
+     } else if (size == 3) {
+         7
+     } else if (size == 4) {
+         4
+     } else if (size == 5) {
+         4
+     } else {
+         0
+     }
+ }
> 
> # Function to get the most frequently occuring words and collocations of words
> # (approximately the top 10% of each class) with the respective counts of
> # occurrences on the provided sample.
> getTop10PercentNGramsWithCounts <- function (tokens, cardinality) {
+     if (cardinality == 1) {
+         dfmat <- dfm(tokens)
+         freq <- textstat_frequency(dfmat)
+         freq %>%
+             head(n = (nrow(freq) %/% 10)) %>%
+             transmute(feature, count = frequency)
+     } else {
+         collocations <- textstat_collocations(tokens,
+                                               min_count = getMinCountForNGramSize(cardinality),
+                                               size = cardinality)
+         collocations %>%
+             arrange(desc(count)) %>%
+             transmute(feature = collocation, count)
+     }
+ }
> 
> scriptStartTime <- proc.time()
> 
> set.seed(54321)
> 
> nGrams <- data.frame(source = factor(),
+                      nGramSize = integer(),
+                      feature = character(),
+                      count = integer())
> 
> numBatches <- 20
> elapsedIdx <- 3
> 
> downloadDataset()
> 
> for (source in c("twitter", "news", "blogs")) {
+     skipped <- skippedLines()
+     
+     for (batch in seq(1, numBatches)) {
+         readingStartTime <- proc.time()
+         
+         batchLines <- loadCorpusSample("en_US",
+                                        source,
+                                        samplingProbability = 1 /
+                                            (numBatches - batch + 1)) %>%
+             normalizeText()
+         batchTokens <- batchLines %>%
+             discardLinesWithAutomaticMessage() %>%
+             discardLinesWithProfanity() %>%
+             corpus() %>%
+             tokens(remove_punct = TRUE)
+         
+         readingElapsedTime <- (proc.time() - readingStartTime)[elapsedIdx]
+         linesRead <- length(batchLines)
+         wordsRead <- sum(sapply(strsplit(batchLines, " "), length))
+         print(paste0("Finished reading and cleaning batch ", batch, "/",
+                      numBatches, " of ", source, " texts. Lines read: ",
+                      linesRead, ". Words read: ", wordsRead, ". Elapsed time: ",
+                      readingElapsedTime))
+ 
+         for (size in seq(1, 5)) {
+             statsStartTime <- proc.time()
+             nGrams <- rbind(nGrams,
+                             cbind(source = as.factor(source),
+                                   nGramSize = size,
+                                   getTop10PercentNGramsWithCounts(batchTokens,
+                                                                   size)))
+             statsElapsedTime <- (proc.time() - statsStartTime)[elapsedIdx]
+             print(paste0("Finished processing ", size, "-grams on batch ",
+                          batch, "/", numBatches, " of ", source,
+                          " texts. Elapsed time: ", statsElapsedTime))
+         }
+     }
+ }
[1] "Finished reading and cleaning batch 1/20 of twitter texts. Lines read: 117925. Words read: 1466691. Elapsed time: 271.586"
[1] "Finished processing 1-grams on batch 1/20 of twitter texts. Elapsed time: 2.22399999999999"
[1] "Finished processing 2-grams on batch 1/20 of twitter texts. Elapsed time: 78.198"
[1] "Finished processing 3-grams on batch 1/20 of twitter texts. Elapsed time: 112.5"
[1] "Finished processing 4-grams on batch 1/20 of twitter texts. Elapsed time: 73.361"
[1] "Finished processing 5-grams on batch 1/20 of twitter texts. Elapsed time: 15.0709999999999"
[1] "Finished reading and cleaning batch 2/20 of twitter texts. Lines read: 117939. Words read: 1469692. Elapsed time: 279.315"
[1] "Finished processing 1-grams on batch 2/20 of twitter texts. Elapsed time: 1.95399999999995"
[1] "Finished processing 2-grams on batch 2/20 of twitter texts. Elapsed time: 78.77"
[1] "Finished processing 3-grams on batch 2/20 of twitter texts. Elapsed time: 113.096"
[1] "Finished processing 4-grams on batch 2/20 of twitter texts. Elapsed time: 75.817"
[1] "Finished processing 5-grams on batch 2/20 of twitter texts. Elapsed time: 16.0079999999998"
[1] "Finished reading and cleaning batch 3/20 of twitter texts. Lines read: 118265. Words read: 1467667. Elapsed time: 280.654"
[1] "Finished processing 1-grams on batch 3/20 of twitter texts. Elapsed time: 1.66799999999989"
[1] "Finished processing 2-grams on batch 3/20 of twitter texts. Elapsed time: 79.1179999999999"
[1] "Finished processing 3-grams on batch 3/20 of twitter texts. Elapsed time: 114.422"
[1] "Finished processing 4-grams on batch 3/20 of twitter texts. Elapsed time: 75.4660000000001"
[1] "Finished processing 5-grams on batch 3/20 of twitter texts. Elapsed time: 16.269"
[1] "Finished reading and cleaning batch 4/20 of twitter texts. Lines read: 118467. Words read: 1476706. Elapsed time: 276.528"
[1] "Finished processing 1-grams on batch 4/20 of twitter texts. Elapsed time: 1.93700000000013"
[1] "Finished processing 2-grams on batch 4/20 of twitter texts. Elapsed time: 79.9780000000001"
[1] "Finished processing 3-grams on batch 4/20 of twitter texts. Elapsed time: 116.211"
[1] "Finished processing 4-grams on batch 4/20 of twitter texts. Elapsed time: 76.7919999999999"
[1] "Finished processing 5-grams on batch 4/20 of twitter texts. Elapsed time: 15.4049999999997"
[1] "Finished reading and cleaning batch 5/20 of twitter texts. Lines read: 118032. Words read: 1472449. Elapsed time: 274.842"
[1] "Finished processing 1-grams on batch 5/20 of twitter texts. Elapsed time: 1.91899999999987"
[1] "Finished processing 2-grams on batch 5/20 of twitter texts. Elapsed time: 79.855"
[1] "Finished processing 3-grams on batch 5/20 of twitter texts. Elapsed time: 113.48"
[1] "Finished processing 4-grams on batch 5/20 of twitter texts. Elapsed time: 75.4450000000002"
[1] "Finished processing 5-grams on batch 5/20 of twitter texts. Elapsed time: 14.971"
[1] "Finished reading and cleaning batch 6/20 of twitter texts. Lines read: 117648. Words read: 1466298. Elapsed time: 272.311"
[1] "Finished processing 1-grams on batch 6/20 of twitter texts. Elapsed time: 1.68499999999995"
[1] "Finished processing 2-grams on batch 6/20 of twitter texts. Elapsed time: 78.9349999999999"
[1] "Finished processing 3-grams on batch 6/20 of twitter texts. Elapsed time: 113.871"
[1] "Finished processing 4-grams on batch 6/20 of twitter texts. Elapsed time: 76.4629999999997"
[1] "Finished processing 5-grams on batch 6/20 of twitter texts. Elapsed time: 16.5450000000001"
[1] "Finished reading and cleaning batch 7/20 of twitter texts. Lines read: 117414. Words read: 1462854. Elapsed time: 273.54"
[1] "Finished processing 1-grams on batch 7/20 of twitter texts. Elapsed time: 1.75399999999991"
[1] "Finished processing 2-grams on batch 7/20 of twitter texts. Elapsed time: 78.2709999999997"
[1] "Finished processing 3-grams on batch 7/20 of twitter texts. Elapsed time: 113.45"
[1] "Finished processing 4-grams on batch 7/20 of twitter texts. Elapsed time: 75.4410000000003"
[1] "Finished processing 5-grams on batch 7/20 of twitter texts. Elapsed time: 15.3089999999997"
[1] "Finished reading and cleaning batch 8/20 of twitter texts. Lines read: 118166. Words read: 1471223. Elapsed time: 266.734"
[1] "Finished processing 1-grams on batch 8/20 of twitter texts. Elapsed time: 1.79500000000007"
[1] "Finished processing 2-grams on batch 8/20 of twitter texts. Elapsed time: 79.1139999999996"
[1] "Finished processing 3-grams on batch 8/20 of twitter texts. Elapsed time: 114.218"
[1] "Finished processing 4-grams on batch 8/20 of twitter texts. Elapsed time: 76.335"
[1] "Finished processing 5-grams on batch 8/20 of twitter texts. Elapsed time: 17.2830000000004"
[1] "Finished reading and cleaning batch 9/20 of twitter texts. Lines read: 117925. Words read: 1463202. Elapsed time: 265.132"
[1] "Finished processing 1-grams on batch 9/20 of twitter texts. Elapsed time: 1.62100000000009"
[1] "Finished processing 2-grams on batch 9/20 of twitter texts. Elapsed time: 78.4070000000002"
[1] "Finished processing 3-grams on batch 9/20 of twitter texts. Elapsed time: 111.529"
[1] "Finished processing 4-grams on batch 9/20 of twitter texts. Elapsed time: 75.366"
[1] "Finished processing 5-grams on batch 9/20 of twitter texts. Elapsed time: 15.6750000000002"
[1] "Finished reading and cleaning batch 10/20 of twitter texts. Lines read: 118222. Words read: 1466492. Elapsed time: 267.397999999999"
[1] "Finished processing 1-grams on batch 10/20 of twitter texts. Elapsed time: 1.75399999999991"
[1] "Finished processing 2-grams on batch 10/20 of twitter texts. Elapsed time: 78.3230000000003"
[1] "Finished processing 3-grams on batch 10/20 of twitter texts. Elapsed time: 114.632"
[1] "Finished processing 4-grams on batch 10/20 of twitter texts. Elapsed time: 75.424"
[1] "Finished processing 5-grams on batch 10/20 of twitter texts. Elapsed time: 16.0010000000002"
[1] "Finished reading and cleaning batch 11/20 of twitter texts. Lines read: 118336. Words read: 1476678. Elapsed time: 267.653"
[1] "Finished processing 1-grams on batch 11/20 of twitter texts. Elapsed time: 1.83300000000054"
[1] "Finished processing 2-grams on batch 11/20 of twitter texts. Elapsed time: 79.9219999999996"
[1] "Finished processing 3-grams on batch 11/20 of twitter texts. Elapsed time: 116.302000000001"
[1] "Finished processing 4-grams on batch 11/20 of twitter texts. Elapsed time: 75.9179999999997"
[1] "Finished processing 5-grams on batch 11/20 of twitter texts. Elapsed time: 15.7829999999994"
[1] "Finished reading and cleaning batch 12/20 of twitter texts. Lines read: 118914. Words read: 1476666. Elapsed time: 267.862"
[1] "Finished processing 1-grams on batch 12/20 of twitter texts. Elapsed time: 1.87400000000071"
[1] "Finished processing 2-grams on batch 12/20 of twitter texts. Elapsed time: 79.8699999999999"
[1] "Finished processing 3-grams on batch 12/20 of twitter texts. Elapsed time: 116.11"
[1] "Finished processing 4-grams on batch 12/20 of twitter texts. Elapsed time: 76.3980000000001"
[1] "Finished processing 5-grams on batch 12/20 of twitter texts. Elapsed time: 16.6489999999994"
[1] "Finished reading and cleaning batch 13/20 of twitter texts. Lines read: 117570. Words read: 1461643. Elapsed time: 262.25"
[1] "Finished processing 1-grams on batch 13/20 of twitter texts. Elapsed time: 1.66599999999926"
[1] "Finished processing 2-grams on batch 13/20 of twitter texts. Elapsed time: 78.277"
[1] "Finished processing 3-grams on batch 13/20 of twitter texts. Elapsed time: 111.607"
[1] "Finished processing 4-grams on batch 13/20 of twitter texts. Elapsed time: 74.143"
[1] "Finished processing 5-grams on batch 13/20 of twitter texts. Elapsed time: 15.6199999999999"
[1] "Finished reading and cleaning batch 14/20 of twitter texts. Lines read: 117843. Words read: 1464809. Elapsed time: 263.051"
[1] "Finished processing 1-grams on batch 14/20 of twitter texts. Elapsed time: 1.55600000000049"
[1] "Finished processing 2-grams on batch 14/20 of twitter texts. Elapsed time: 78.0029999999997"
[1] "Finished processing 3-grams on batch 14/20 of twitter texts. Elapsed time: 112.764"
[1] "Finished processing 4-grams on batch 14/20 of twitter texts. Elapsed time: 74.7330000000002"
[1] "Finished processing 5-grams on batch 14/20 of twitter texts. Elapsed time: 16.0299999999997"
[1] "Finished reading and cleaning batch 15/20 of twitter texts. Lines read: 117860. Words read: 1471389. Elapsed time: 262.467"
[1] "Finished processing 1-grams on batch 15/20 of twitter texts. Elapsed time: 1.8179999999993"
[1] "Finished processing 2-grams on batch 15/20 of twitter texts. Elapsed time: 79.25"
[1] "Finished processing 3-grams on batch 15/20 of twitter texts. Elapsed time: 113.921"
[1] "Finished processing 4-grams on batch 15/20 of twitter texts. Elapsed time: 75.7170000000006"
[1] "Finished processing 5-grams on batch 15/20 of twitter texts. Elapsed time: 15.7169999999987"
[1] "Finished reading and cleaning batch 16/20 of twitter texts. Lines read: 117644. Words read: 1464959. Elapsed time: 261.610000000001"
[1] "Finished processing 1-grams on batch 16/20 of twitter texts. Elapsed time: 1.57699999999932"
[1] "Finished processing 2-grams on batch 16/20 of twitter texts. Elapsed time: 78.3590000000004"
[1] "Finished processing 3-grams on batch 16/20 of twitter texts. Elapsed time: 112.697"
[1] "Finished processing 4-grams on batch 16/20 of twitter texts. Elapsed time: 74.1219999999994"
[1] "Finished processing 5-grams on batch 16/20 of twitter texts. Elapsed time: 15.3469999999998"
[1] "Finished reading and cleaning batch 17/20 of twitter texts. Lines read: 117754. Words read: 1464419. Elapsed time: 252.767"
[1] "Finished processing 1-grams on batch 17/20 of twitter texts. Elapsed time: 1.66200000000026"
[1] "Finished processing 2-grams on batch 17/20 of twitter texts. Elapsed time: 77.9360000000015"
[1] "Finished processing 3-grams on batch 17/20 of twitter texts. Elapsed time: 112.531999999999"
[1] "Finished processing 4-grams on batch 17/20 of twitter texts. Elapsed time: 75.8549999999996"
[1] "Finished processing 5-grams on batch 17/20 of twitter texts. Elapsed time: 16.0540000000001"
[1] "Finished reading and cleaning batch 18/20 of twitter texts. Lines read: 117965. Words read: 1469915. Elapsed time: 252.906000000001"
[1] "Finished processing 1-grams on batch 18/20 of twitter texts. Elapsed time: 1.5679999999993"
[1] "Finished processing 2-grams on batch 18/20 of twitter texts. Elapsed time: 79.094000000001"
[1] "Finished processing 3-grams on batch 18/20 of twitter texts. Elapsed time: 114.770999999999"
[1] "Finished processing 4-grams on batch 18/20 of twitter texts. Elapsed time: 75.2749999999996"
[1] "Finished processing 5-grams on batch 18/20 of twitter texts. Elapsed time: 15.9990000000016"
[1] "Finished reading and cleaning batch 19/20 of twitter texts. Lines read: 118029. Words read: 1465881. Elapsed time: 253.020999999999"
[1] "Finished processing 1-grams on batch 19/20 of twitter texts. Elapsed time: 1.86100000000079"
[1] "Finished processing 2-grams on batch 19/20 of twitter texts. Elapsed time: 78.1540000000005"
[1] "Finished processing 3-grams on batch 19/20 of twitter texts. Elapsed time: 112.681999999999"
[1] "Finished processing 4-grams on batch 19/20 of twitter texts. Elapsed time: 75.2870000000003"
[1] "Finished processing 5-grams on batch 19/20 of twitter texts. Elapsed time: 15.1319999999996"
[1] "Finished reading and cleaning batch 20/20 of twitter texts. Lines read: 118230. Words read: 1471430. Elapsed time: 253.266"
[1] "Finished processing 1-grams on batch 20/20 of twitter texts. Elapsed time: 1.61299999999937"
[1] "Finished processing 2-grams on batch 20/20 of twitter texts. Elapsed time: 78.5409999999993"
[1] "Finished processing 3-grams on batch 20/20 of twitter texts. Elapsed time: 113.165000000001"
[1] "Finished processing 4-grams on batch 20/20 of twitter texts. Elapsed time: 75.0659999999989"
[1] "Finished processing 5-grams on batch 20/20 of twitter texts. Elapsed time: 14.902"
[1] "Finished reading and cleaning batch 1/20 of news texts. Lines read: 50849. Words read: 1686501. Elapsed time: 134.448"
[1] "Finished processing 1-grams on batch 1/20 of news texts. Elapsed time: 1.32599999999911"
[1] "Finished processing 2-grams on batch 1/20 of news texts. Elapsed time: 134.541999999999"
[1] "Finished processing 3-grams on batch 1/20 of news texts. Elapsed time: 177.639000000001"
[1] "Finished processing 4-grams on batch 1/20 of news texts. Elapsed time: 110.41"
[1] "Finished processing 5-grams on batch 1/20 of news texts. Elapsed time: 21.6720000000005"
[1] "Finished reading and cleaning batch 2/20 of news texts. Lines read: 50828. Words read: 1682610. Elapsed time: 132.216999999999"
[1] "Finished processing 1-grams on batch 2/20 of news texts. Elapsed time: 1.3640000000014"
[1] "Finished processing 2-grams on batch 2/20 of news texts. Elapsed time: 132.602999999999"
[1] "Finished processing 3-grams on batch 2/20 of news texts. Elapsed time: 176.763999999999"
[1] "Finished processing 4-grams on batch 2/20 of news texts. Elapsed time: 113.975"
[1] "Finished processing 5-grams on batch 2/20 of news texts. Elapsed time: 20.8770000000004"
[1] "Finished reading and cleaning batch 3/20 of news texts. Lines read: 50529. Words read: 1677639. Elapsed time: 131.386"
[1] "Finished processing 1-grams on batch 3/20 of news texts. Elapsed time: 1.38700000000063"
[1] "Finished processing 2-grams on batch 3/20 of news texts. Elapsed time: 133.030999999999"
[1] "Finished processing 3-grams on batch 3/20 of news texts. Elapsed time: 176.175000000001"
[1] "Finished processing 4-grams on batch 3/20 of news texts. Elapsed time: 110.094999999999"
[1] "Finished processing 5-grams on batch 3/20 of news texts. Elapsed time: 20.268"
[1] "Finished reading and cleaning batch 4/20 of news texts. Lines read: 50325. Words read: 1657675. Elapsed time: 130.809999999999"
[1] "Finished processing 1-grams on batch 4/20 of news texts. Elapsed time: 1.34200000000055"
[1] "Finished processing 2-grams on batch 4/20 of news texts. Elapsed time: 131.357"
[1] "Finished processing 3-grams on batch 4/20 of news texts. Elapsed time: 173.085999999999"
[1] "Finished processing 4-grams on batch 4/20 of news texts. Elapsed time: 105.225"
[1] "Finished processing 5-grams on batch 4/20 of news texts. Elapsed time: 19.2029999999995"
[1] "Finished reading and cleaning batch 5/20 of news texts. Lines read: 50535. Words read: 1668754. Elapsed time: 130.585999999999"
[1] "Finished processing 1-grams on batch 5/20 of news texts. Elapsed time: 1.23500000000058"
[1] "Finished processing 2-grams on batch 5/20 of news texts. Elapsed time: 131.171"
[1] "Finished processing 3-grams on batch 5/20 of news texts. Elapsed time: 174.347"
[1] "Finished processing 4-grams on batch 5/20 of news texts. Elapsed time: 107.657999999999"
[1] "Finished processing 5-grams on batch 5/20 of news texts. Elapsed time: 19.3559999999998"
[1] "Finished reading and cleaning batch 6/20 of news texts. Lines read: 50382. Words read: 1673626. Elapsed time: 130.617"
[1] "Finished processing 1-grams on batch 6/20 of news texts. Elapsed time: 1.55600000000049"
[1] "Finished processing 2-grams on batch 6/20 of news texts. Elapsed time: 133.219999999999"
[1] "Finished processing 3-grams on batch 6/20 of news texts. Elapsed time: 177.331"
[1] "Finished processing 4-grams on batch 6/20 of news texts. Elapsed time: 108.011"
[1] "Finished processing 5-grams on batch 6/20 of news texts. Elapsed time: 19.1839999999993"
[1] "Finished reading and cleaning batch 7/20 of news texts. Lines read: 50350. Words read: 1667275. Elapsed time: 130.409"
[1] "Finished processing 1-grams on batch 7/20 of news texts. Elapsed time: 1.36299999999937"
[1] "Finished processing 2-grams on batch 7/20 of news texts. Elapsed time: 132.594000000001"
[1] "Finished processing 3-grams on batch 7/20 of news texts. Elapsed time: 174.307000000001"
[1] "Finished processing 4-grams on batch 7/20 of news texts. Elapsed time: 108.589999999998"
[1] "Finished processing 5-grams on batch 7/20 of news texts. Elapsed time: 18.8900000000012"
[1] "Finished reading and cleaning batch 8/20 of news texts. Lines read: 50501. Words read: 1677558. Elapsed time: 130.687"
[1] "Finished processing 1-grams on batch 8/20 of news texts. Elapsed time: 1.38800000000083"
[1] "Finished processing 2-grams on batch 8/20 of news texts. Elapsed time: 132.914999999999"
[1] "Finished processing 3-grams on batch 8/20 of news texts. Elapsed time: 178.367"
[1] "Finished processing 4-grams on batch 8/20 of news texts. Elapsed time: 107.751"
[1] "Finished processing 5-grams on batch 8/20 of news texts. Elapsed time: 20.0829999999987"
[1] "Finished reading and cleaning batch 9/20 of news texts. Lines read: 50777. Words read: 1675912. Elapsed time: 133.093000000001"
[1] "Finished processing 1-grams on batch 9/20 of news texts. Elapsed time: 1.43299999999908"
[1] "Finished processing 2-grams on batch 9/20 of news texts. Elapsed time: 133.233"
[1] "Finished processing 3-grams on batch 9/20 of news texts. Elapsed time: 175.351000000001"
[1] "Finished processing 4-grams on batch 9/20 of news texts. Elapsed time: 108.462"
[1] "Finished processing 5-grams on batch 9/20 of news texts. Elapsed time: 19.4850000000006"
[1] "Finished reading and cleaning batch 10/20 of news texts. Lines read: 50570. Words read: 1677697. Elapsed time: 131.092999999999"
[1] "Finished processing 1-grams on batch 10/20 of news texts. Elapsed time: 1.43900000000031"
[1] "Finished processing 2-grams on batch 10/20 of news texts. Elapsed time: 133.583999999999"
[1] "Finished processing 3-grams on batch 10/20 of news texts. Elapsed time: 177.409"
[1] "Finished processing 4-grams on batch 10/20 of news texts. Elapsed time: 109.822"
[1] "Finished processing 5-grams on batch 10/20 of news texts. Elapsed time: 20.5910000000003"
[1] "Finished reading and cleaning batch 11/20 of news texts. Lines read: 50877. Words read: 1681028. Elapsed time: 130.577000000001"
[1] "Finished processing 1-grams on batch 11/20 of news texts. Elapsed time: 1.52199999999721"
[1] "Finished processing 2-grams on batch 11/20 of news texts. Elapsed time: 134.43"
[1] "Finished processing 3-grams on batch 11/20 of news texts. Elapsed time: 178.339"
[1] "Finished processing 4-grams on batch 11/20 of news texts. Elapsed time: 113.753000000001"
[1] "Finished processing 5-grams on batch 11/20 of news texts. Elapsed time: 22.7710000000006"
[1] "Finished reading and cleaning batch 12/20 of news texts. Lines read: 50005. Words read: 1652001. Elapsed time: 128.121999999999"
[1] "Finished processing 1-grams on batch 12/20 of news texts. Elapsed time: 1.30099999999948"
[1] "Finished processing 2-grams on batch 12/20 of news texts. Elapsed time: 129.357"
[1] "Finished processing 3-grams on batch 12/20 of news texts. Elapsed time: 170.201000000001"
[1] "Finished processing 4-grams on batch 12/20 of news texts. Elapsed time: 103.662"
[1] "Finished processing 5-grams on batch 12/20 of news texts. Elapsed time: 17.9799999999996"
[1] "Finished reading and cleaning batch 13/20 of news texts. Lines read: 50299. Words read: 1667915. Elapsed time: 129.043999999998"
[1] "Finished processing 1-grams on batch 13/20 of news texts. Elapsed time: 1.38999999999942"
[1] "Finished processing 2-grams on batch 13/20 of news texts. Elapsed time: 132.018"
[1] "Finished processing 3-grams on batch 13/20 of news texts. Elapsed time: 172.574000000001"
[1] "Finished processing 4-grams on batch 13/20 of news texts. Elapsed time: 107.388999999999"
[1] "Finished processing 5-grams on batch 13/20 of news texts. Elapsed time: 19.0089999999982"
[1] "Finished reading and cleaning batch 14/20 of news texts. Lines read: 50460. Words read: 1676971. Elapsed time: 130.010000000002"
[1] "Finished processing 1-grams on batch 14/20 of news texts. Elapsed time: 1.40699999999924"
[1] "Finished processing 2-grams on batch 14/20 of news texts. Elapsed time: 134.437000000002"
[1] "Finished processing 3-grams on batch 14/20 of news texts. Elapsed time: 178.492999999999"
[1] "Finished processing 4-grams on batch 14/20 of news texts. Elapsed time: 110.223000000002"
[1] "Finished processing 5-grams on batch 14/20 of news texts. Elapsed time: 20.0929999999971"
[1] "Finished reading and cleaning batch 15/20 of news texts. Lines read: 50510. Words read: 1679677. Elapsed time: 130.200000000001"
[1] "Finished processing 1-grams on batch 15/20 of news texts. Elapsed time: 1.43799999999828"
[1] "Finished processing 2-grams on batch 15/20 of news texts. Elapsed time: 136.017"
[1] "Finished processing 3-grams on batch 15/20 of news texts. Elapsed time: 175.758000000002"
[1] "Finished processing 4-grams on batch 15/20 of news texts. Elapsed time: 110.375"
[1] "Finished processing 5-grams on batch 15/20 of news texts. Elapsed time: 19.0370000000003"
[1] "Finished reading and cleaning batch 16/20 of news texts. Lines read: 50331. Words read: 1669110. Elapsed time: 128.638000000003"
[1] "Finished processing 1-grams on batch 16/20 of news texts. Elapsed time: 1.62800000000061"
[1] "Finished processing 2-grams on batch 16/20 of news texts. Elapsed time: 131.769"
[1] "Finished processing 3-grams on batch 16/20 of news texts. Elapsed time: 174.116999999998"
[1] "Finished processing 4-grams on batch 16/20 of news texts. Elapsed time: 108.914000000001"
[1] "Finished processing 5-grams on batch 16/20 of news texts. Elapsed time: 21.3160000000025"
[1] "Finished reading and cleaning batch 17/20 of news texts. Lines read: 50363. Words read: 1670966. Elapsed time: 129.069000000003"
[1] "Finished processing 1-grams on batch 17/20 of news texts. Elapsed time: 1.42099999999846"
[1] "Finished processing 2-grams on batch 17/20 of news texts. Elapsed time: 132.677"
[1] "Finished processing 3-grams on batch 17/20 of news texts. Elapsed time: 174.940000000002"
[1] "Finished processing 4-grams on batch 17/20 of news texts. Elapsed time: 108.131999999998"
[1] "Finished processing 5-grams on batch 17/20 of news texts. Elapsed time: 19.1549999999988"
[1] "Finished reading and cleaning batch 18/20 of news texts. Lines read: 50200. Words read: 1662591. Elapsed time: 129.114000000001"
[1] "Finished processing 1-grams on batch 18/20 of news texts. Elapsed time: 1.65299999999843"
[1] "Finished processing 2-grams on batch 18/20 of news texts. Elapsed time: 131.498"
[1] "Finished processing 3-grams on batch 18/20 of news texts. Elapsed time: 173.368000000002"
[1] "Finished processing 4-grams on batch 18/20 of news texts. Elapsed time: 107.771999999997"
[1] "Finished processing 5-grams on batch 18/20 of news texts. Elapsed time: 19.2260000000024"
[1] "Finished reading and cleaning batch 19/20 of news texts. Lines read: 50787. Words read: 1683870. Elapsed time: 129.575000000001"
[1] "Finished processing 1-grams on batch 19/20 of news texts. Elapsed time: 1.30299999999988"
[1] "Finished processing 2-grams on batch 19/20 of news texts. Elapsed time: 139.995999999999"
[1] "Finished processing 3-grams on batch 19/20 of news texts. Elapsed time: 180.603999999999"
[1] "Finished processing 4-grams on batch 19/20 of news texts. Elapsed time: 112.667000000001"
[1] "Finished processing 5-grams on batch 19/20 of news texts. Elapsed time: 20.9929999999986"
[1] "Finished reading and cleaning batch 20/20 of news texts. Lines read: 50764. Words read: 1686879. Elapsed time: 130.529000000002"
[1] "Finished processing 1-grams on batch 20/20 of news texts. Elapsed time: 1.65699999999924"
[1] "Finished processing 2-grams on batch 20/20 of news texts. Elapsed time: 134.958999999999"
[1] "Finished processing 3-grams on batch 20/20 of news texts. Elapsed time: 180.348000000002"
[1] "Finished processing 4-grams on batch 20/20 of news texts. Elapsed time: 114.039999999997"
[1] "Finished processing 5-grams on batch 20/20 of news texts. Elapsed time: 20.1779999999999"
[1] "Finished reading and cleaning batch 1/20 of blogs texts. Lines read: 44806. Words read: 1836243. Elapsed time: 124.312000000002"
[1] "Finished processing 1-grams on batch 1/20 of blogs texts. Elapsed time: 1.17299999999886"
[1] "Finished processing 2-grams on batch 1/20 of blogs texts. Elapsed time: 139.558000000001"
[1] "Finished processing 3-grams on batch 1/20 of blogs texts. Elapsed time: 245.537"
[1] "Finished processing 4-grams on batch 1/20 of blogs texts. Elapsed time: 160.167000000001"
[1] "Finished processing 5-grams on batch 1/20 of blogs texts. Elapsed time: 26.6979999999967"
[1] "Finished reading and cleaning batch 2/20 of blogs texts. Lines read: 45005. Words read: 1856626. Elapsed time: 125.442000000003"
[1] "Finished processing 1-grams on batch 2/20 of blogs texts. Elapsed time: 1.28700000000026"
[1] "Finished processing 2-grams on batch 2/20 of blogs texts. Elapsed time: 141.014999999999"
[1] "Finished processing 3-grams on batch 2/20 of blogs texts. Elapsed time: 246.16"
[1] "Finished processing 4-grams on batch 2/20 of blogs texts. Elapsed time: 166.815999999999"
[1] "Finished processing 5-grams on batch 2/20 of blogs texts. Elapsed time: 28.1190000000024"
[1] "Finished reading and cleaning batch 3/20 of blogs texts. Lines read: 45079. Words read: 1853239. Elapsed time: 124.189999999999"
[1] "Finished processing 1-grams on batch 3/20 of blogs texts. Elapsed time: 1.49199999999837"
[1] "Finished processing 2-grams on batch 3/20 of blogs texts. Elapsed time: 140.258000000002"
[1] "Finished processing 3-grams on batch 3/20 of blogs texts. Elapsed time: 249.117999999999"
[1] "Finished processing 4-grams on batch 3/20 of blogs texts. Elapsed time: 166.341"
[1] "Finished processing 5-grams on batch 3/20 of blogs texts. Elapsed time: 29.5499999999993"
[1] "Finished reading and cleaning batch 4/20 of blogs texts. Lines read: 45076. Words read: 1832869. Elapsed time: 125.003000000001"
[1] "Finished processing 1-grams on batch 4/20 of blogs texts. Elapsed time: 1.30500000000029"
[1] "Finished processing 2-grams on batch 4/20 of blogs texts. Elapsed time: 138.821"
[1] "Finished processing 3-grams on batch 4/20 of blogs texts. Elapsed time: 242.262999999999"
[1] "Finished processing 4-grams on batch 4/20 of blogs texts. Elapsed time: 159.583999999999"
[1] "Finished processing 5-grams on batch 4/20 of blogs texts. Elapsed time: 27.3359999999993"
[1] "Finished reading and cleaning batch 5/20 of blogs texts. Lines read: 45272. Words read: 1856112. Elapsed time: 125.18"
[1] "Finished processing 1-grams on batch 5/20 of blogs texts. Elapsed time: 1.37000000000262"
[1] "Finished processing 2-grams on batch 5/20 of blogs texts. Elapsed time: 140.738999999998"
[1] "Finished processing 3-grams on batch 5/20 of blogs texts. Elapsed time: 244.538"
[1] "Finished processing 4-grams on batch 5/20 of blogs texts. Elapsed time: 160.542000000001"
[1] "Finished processing 5-grams on batch 5/20 of blogs texts. Elapsed time: 27.525999999998"
[1] "Finished reading and cleaning batch 6/20 of blogs texts. Lines read: 44674. Words read: 1823997. Elapsed time: 123.054"
[1] "Finished processing 1-grams on batch 6/20 of blogs texts. Elapsed time: 1.48300000000017"
[1] "Finished processing 2-grams on batch 6/20 of blogs texts. Elapsed time: 137.924999999999"
[1] "Finished processing 3-grams on batch 6/20 of blogs texts. Elapsed time: 236.553"
[1] "Finished processing 4-grams on batch 6/20 of blogs texts. Elapsed time: 152.244000000002"
[1] "Finished processing 5-grams on batch 6/20 of blogs texts. Elapsed time: 24.3259999999973"
[1] "Finished reading and cleaning batch 7/20 of blogs texts. Lines read: 44719. Words read: 1843275. Elapsed time: 123.708000000002"
[1] "Finished processing 1-grams on batch 7/20 of blogs texts. Elapsed time: 1.20499999999811"
[1] "Finished processing 2-grams on batch 7/20 of blogs texts. Elapsed time: 142.414000000001"
[1] "Finished processing 3-grams on batch 7/20 of blogs texts. Elapsed time: 243.885000000002"
[1] "Finished processing 4-grams on batch 7/20 of blogs texts. Elapsed time: 160.530999999999"
[1] "Finished processing 5-grams on batch 7/20 of blogs texts. Elapsed time: 27.4069999999992"
[1] "Finished reading and cleaning batch 8/20 of blogs texts. Lines read: 45150. Words read: 1846625. Elapsed time: 124.305"
[1] "Finished processing 1-grams on batch 8/20 of blogs texts. Elapsed time: 1.37000000000262"
[1] "Finished processing 2-grams on batch 8/20 of blogs texts. Elapsed time: 139.130999999998"
[1] "Finished processing 3-grams on batch 8/20 of blogs texts. Elapsed time: 245.266"
[1] "Finished processing 4-grams on batch 8/20 of blogs texts. Elapsed time: 159.809000000001"
[1] "Finished processing 5-grams on batch 8/20 of blogs texts. Elapsed time: 28.3359999999993"
[1] "Finished reading and cleaning batch 9/20 of blogs texts. Lines read: 45110. Words read: 1853890. Elapsed time: 124.530999999999"
[1] "Finished processing 1-grams on batch 9/20 of blogs texts. Elapsed time: 1.24299999999857"
[1] "Finished processing 2-grams on batch 9/20 of blogs texts. Elapsed time: 140.205000000002"
[1] "Finished processing 3-grams on batch 9/20 of blogs texts. Elapsed time: 246.966"
[1] "Finished processing 4-grams on batch 9/20 of blogs texts. Elapsed time: 162.242999999999"
[1] "Finished processing 5-grams on batch 9/20 of blogs texts. Elapsed time: 26.9150000000009"
[1] "Finished reading and cleaning batch 10/20 of blogs texts. Lines read: 44498. Words read: 1829662. Elapsed time: 122.605"
[1] "Finished processing 1-grams on batch 10/20 of blogs texts. Elapsed time: 1.31400000000212"
[1] "Finished processing 2-grams on batch 10/20 of blogs texts. Elapsed time: 136.602999999999"
[1] "Finished processing 3-grams on batch 10/20 of blogs texts. Elapsed time: 238.219000000001"
[1] "Finished processing 4-grams on batch 10/20 of blogs texts. Elapsed time: 156.637999999999"
[1] "Finished processing 5-grams on batch 10/20 of blogs texts. Elapsed time: 27.2909999999974"
[1] "Finished reading and cleaning batch 11/20 of blogs texts. Lines read: 45047. Words read: 1844222. Elapsed time: 124.332000000002"
[1] "Finished processing 1-grams on batch 11/20 of blogs texts. Elapsed time: 1.59699999999793"
[1] "Finished processing 2-grams on batch 11/20 of blogs texts. Elapsed time: 138.811000000002"
[1] "Finished processing 3-grams on batch 11/20 of blogs texts. Elapsed time: 241.403999999999"
[1] "Finished processing 4-grams on batch 11/20 of blogs texts. Elapsed time: 159"
[1] "Finished processing 5-grams on batch 11/20 of blogs texts. Elapsed time: 26.3090000000011"
[1] "Finished reading and cleaning batch 12/20 of blogs texts. Lines read: 44767. Words read: 1835790. Elapsed time: 121.978999999999"
[1] "Finished processing 1-grams on batch 12/20 of blogs texts. Elapsed time: 1.50399999999718"
[1] "Finished processing 2-grams on batch 12/20 of blogs texts. Elapsed time: 138.353000000003"
[1] "Finished processing 3-grams on batch 12/20 of blogs texts. Elapsed time: 240.074000000001"
[1] "Finished processing 4-grams on batch 12/20 of blogs texts. Elapsed time: 156.131999999998"
[1] "Finished processing 5-grams on batch 12/20 of blogs texts. Elapsed time: 25.8590000000004"
[1] "Finished reading and cleaning batch 13/20 of blogs texts. Lines read: 45314. Words read: 1851461. Elapsed time: 124.189999999999"
[1] "Finished processing 1-grams on batch 13/20 of blogs texts. Elapsed time: 1.35699999999997"
[1] "Finished processing 2-grams on batch 13/20 of blogs texts. Elapsed time: 140.189999999999"
[1] "Finished processing 3-grams on batch 13/20 of blogs texts. Elapsed time: 244.369999999999"
[1] "Finished processing 4-grams on batch 13/20 of blogs texts. Elapsed time: 160.709000000003"
[1] "Finished processing 5-grams on batch 13/20 of blogs texts. Elapsed time: 27.0599999999977"
[1] "Finished reading and cleaning batch 14/20 of blogs texts. Lines read: 44940. Words read: 1826118. Elapsed time: 122.509000000002"
[1] "Finished processing 1-grams on batch 14/20 of blogs texts. Elapsed time: 1.34100000000035"
[1] "Finished processing 2-grams on batch 14/20 of blogs texts. Elapsed time: 137.471999999998"
[1] "Finished processing 3-grams on batch 14/20 of blogs texts. Elapsed time: 238.209000000003"
[1] "Finished processing 4-grams on batch 14/20 of blogs texts. Elapsed time: 154.587"
[1] "Finished processing 5-grams on batch 14/20 of blogs texts. Elapsed time: 24.9099999999999"
[1] "Finished reading and cleaning batch 15/20 of blogs texts. Lines read: 45206. Words read: 1831855. Elapsed time: 123.055"
[1] "Finished processing 1-grams on batch 15/20 of blogs texts. Elapsed time: 1.31000000000131"
[1] "Finished processing 2-grams on batch 15/20 of blogs texts. Elapsed time: 137.414000000001"
[1] "Finished processing 3-grams on batch 15/20 of blogs texts. Elapsed time: 241.127"
[1] "Finished processing 4-grams on batch 15/20 of blogs texts. Elapsed time: 159.150000000001"
[1] "Finished processing 5-grams on batch 15/20 of blogs texts. Elapsed time: 26.7569999999978"
[1] "Finished reading and cleaning batch 16/20 of blogs texts. Lines read: 44834. Words read: 1833093. Elapsed time: 123.476999999999"
[1] "Finished processing 1-grams on batch 16/20 of blogs texts. Elapsed time: 1.36000000000058"
[1] "Finished processing 2-grams on batch 16/20 of blogs texts. Elapsed time: 137.987999999998"
[1] "Finished processing 3-grams on batch 16/20 of blogs texts. Elapsed time: 241.372000000003"
[1] "Finished processing 4-grams on batch 16/20 of blogs texts. Elapsed time: 156.796000000002"
[1] "Finished processing 5-grams on batch 16/20 of blogs texts. Elapsed time: 26.3139999999985"
[1] "Finished reading and cleaning batch 17/20 of blogs texts. Lines read: 44889. Words read: 1839426. Elapsed time: 122.897000000004"
[1] "Finished processing 1-grams on batch 17/20 of blogs texts. Elapsed time: 1.34999999999854"
[1] "Finished processing 2-grams on batch 17/20 of blogs texts. Elapsed time: 138.739999999998"
[1] "Finished processing 3-grams on batch 17/20 of blogs texts. Elapsed time: 242.540999999997"
[1] "Finished processing 4-grams on batch 17/20 of blogs texts. Elapsed time: 159.535000000003"
[1] "Finished processing 5-grams on batch 17/20 of blogs texts. Elapsed time: 27.2339999999967"
[1] "Finished reading and cleaning batch 18/20 of blogs texts. Lines read: 45076. Words read: 1837560. Elapsed time: 122.465000000004"
[1] "Finished processing 1-grams on batch 18/20 of blogs texts. Elapsed time: 1.27700000000186"
[1] "Finished processing 2-grams on batch 18/20 of blogs texts. Elapsed time: 138.678"
[1] "Finished processing 3-grams on batch 18/20 of blogs texts. Elapsed time: 242.334999999999"
[1] "Finished processing 4-grams on batch 18/20 of blogs texts. Elapsed time: 161.080999999998"
[1] "Finished processing 5-grams on batch 18/20 of blogs texts. Elapsed time: 25.9110000000001"
[1] "Finished reading and cleaning batch 19/20 of blogs texts. Lines read: 44906. Words read: 1848853. Elapsed time: 121.989999999998"
[1] "Finished processing 1-grams on batch 19/20 of blogs texts. Elapsed time: 1.3179999999993"
[1] "Finished processing 2-grams on batch 19/20 of blogs texts. Elapsed time: 140.612999999998"
[1] "Finished processing 3-grams on batch 19/20 of blogs texts. Elapsed time: 246.206999999995"
[1] "Finished processing 4-grams on batch 19/20 of blogs texts. Elapsed time: 163.243000000002"
[1] "Finished processing 5-grams on batch 19/20 of blogs texts. Elapsed time: 28.0290000000023"
[1] "Finished reading and cleaning batch 20/20 of blogs texts. Lines read: 44920. Words read: 1845786. Elapsed time: 122.385999999999"
[1] "Finished processing 1-grams on batch 20/20 of blogs texts. Elapsed time: 1.31500000000233"
[1] "Finished processing 2-grams on batch 20/20 of blogs texts. Elapsed time: 139.771000000001"
[1] "Finished processing 3-grams on batch 20/20 of blogs texts. Elapsed time: 241.566999999995"
[1] "Finished processing 4-grams on batch 20/20 of blogs texts. Elapsed time: 160.437000000005"
[1] "Finished processing 5-grams on batch 20/20 of blogs texts. Elapsed time: 26.9609999999957"
> 
> # Consolidate counts of common collocations appearing in different samples
> nGramsSummarized <- nGrams %>%
+     group_by(source, nGramSize, feature) %>%
+     summarize(count = sum(count)) %>%
+     arrange(source, nGramSize, desc(count))
> 
> write.csv(nGramsSummarized,
+           file = 'word-prediction-dataset.csv',
+           row.names = FALSE)
> 
> totalElapsedTime <- (proc.time() - scriptStartTime)[elapsedIdx]
> 
> print(paste("Total elapsed time:", totalElapsedTime))
[1] "Total elapsed time: 36401.346"
> 
> 
> nGramsLoaded <- read.csv(file = 'word-prediction-dataset.csv',
+                    colClasses = c("factor", "integer", "character", "integer"))
> 
> 
> proc.time()
     user    system   elapsed 
61800.420    17.994 36403.279 
