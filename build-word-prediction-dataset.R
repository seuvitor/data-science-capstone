library(data.table)
library(textclean)
library(dplyr)
library(quanteda)

# Function to download the dataset if it is not present on the directory.
downloadDataset <- function() {
    datasetFileName <- "Coursera-SwiftKey.zip"
    
    datasetUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    
    if (!file.exists(datasetFileName)) {
        download.file(datasetUrl, datasetFileName)
    }
    
    if (!file.exists("final")) {
        unzip(datasetFileName)
    }
}

# Function to get all the texts from the SwiftKey raw text dataset, scrambled.
getFullCorpusPermutation <- function() {
    lines <- character()
    
    filePath <- "final/en_US/en_US.twitter.txt"
    lines <- c(lines, fread(filePath, sep = NULL, header = FALSE, col.names = "text", blank.lines.skip = TRUE)$text)
    
    filePath <- "final/en_US/en_US.news.txt"
    lines <- c(lines, fread(filePath, sep = NULL, header = FALSE, col.names = "text", blank.lines.skip = TRUE, nrows = 987096)$text)
    #Skip line 987097 which contains invalid symbol
    lines <- c(lines, fread(filePath, sep = NULL, header = FALSE, col.names = "text", blank.lines.skip = TRUE, skip = 987097)$text)
    
    filePath <- "final/en_US/en_US.blogs.txt"
    lines <- c(lines, fread(filePath, sep = NULL, header = FALSE, col.names = "text", blank.lines.skip = TRUE, nrows = 615491)$text)
    #Skip line 615492 which contains invalid symbol
    lines <- c(lines, fread(filePath, sep = NULL, header = FALSE, col.names = "text", blank.lines.skip = TRUE, skip = 615492, nrows =(741885 - 615492 - 1))$text)
    #Skip line 741885 which contains invalid symbol
    lines <- c(lines, fread(filePath, sep = NULL, header = FALSE, col.names = "text", blank.lines.skip = TRUE, skip = 741885)$text)
    
    sample(lines)
}

# Function to get a partition of the texts.
getCorpusPartition <- function(partition, numPartitions) {
    set.seed(76543)
    lines <- getFullCorpusPermutation()
    startOfPartition <- round((partition - 1) * (length(lines) / numPartitions)) + 1
    endOfPartition <- round(partition * (length(lines) / numPartitions))
    lines[startOfPartition:endOfPartition]
}

removeNonAsciiCharacters <- function(lines) {
    gsub('[^ -~]', '', lines)
}

# Very basic text normalization to apply to the raw data.
normalizeText <- function (lines) {
    char_tolower(removeNonAsciiCharacters(lines))
}

# Gets a list of words to use as reference when discarding profanity from text.
getBadWords <- function() {
    badWordsFileName <- "badwords-en.txt"
    badWordsUrl <- "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
    if (!file.exists(badWordsFileName)) {
        download.file(badWordsUrl, badWordsFileName)
    }
    readLines(badWordsFileName)
}

# Keeps the lines of text that do not contain profanity.
discardLinesWithProfanity <- function (lines) {
    badWords <- getBadWords()
    badWordsExp <- paste("\\b(", paste(badWords, collapse = "|"), ")\\b")
    drop_element_regex(lines, badWordsExp)
}

# Discard lines of text that appear to be automatically generated by bots on
# twitter.
discardLinesWithAutomaticMessage <- function (lines) {
    automatic <- c("thanks for the follow",
                   "thank you for the follow",
                   "thanks for the ff",
                   "thanks for the rt",
                   "thank you for the rt",
                   "thanks for the mention",
                   "thanks for the shout out")
    automaticExp <- paste("\\b(", paste(automatic, collapse = "|"), ")\\b")
    drop_element_regex(lines, automaticExp)
}

# Function to get the words and collocations of words that occur at least 4 times on the lines
# of text with the respective counts of occurrences on the provided sample.
getTop10PercentNGramsWithCounts <- function (lines, cardinality) {
    minimumCount <- 4
    nGramDfm <- dfm_trim(dfm(lines, ngrams = cardinality, remove_numbers = TRUE,
                             remove_punct = TRUE, remove_symbols = TRUE, remove_separators = TRUE,
                             remove_twitter = TRUE, remove_hyphens = TRUE),
                         min_termfreq = minimumCount)
    nGramCounts <- sort(colSums(nGramDfm), decreasing = TRUE)
    data.table(feature = gsub(names(nGramCounts), pattern = "_", replacement = " "),
               count = nGramCounts)
}

scriptStartTime <- proc.time()

set.seed(54321)

nGrams <- data.frame(nGramSize = integer(),
                     feature = character(),
                     count = integer())

numBatches <- 16
elapsedIdx <- 3

downloadDataset()

for (batch in seq(1, numBatches)) {
    batchLines <- getCorpusPartition(batch, numBatches) %>%
        normalizeText() %>%
        discardLinesWithAutomaticMessage() %>%
        discardLinesWithProfanity()
    for (size in seq(1, 5)) {
        statsStartTime <- proc.time()
        nGrams <- rbind(nGrams,
                        cbind(nGramSize = size,
                              getTop10PercentNGramsWithCounts(batchLines, size)))
        statsElapsedTime <- (proc.time() - statsStartTime)[elapsedIdx]
        print(paste0("Finished processing ", size, "-grams on batch ",
                     batch, "/", numBatches, ". Elapsed time: ", statsElapsedTime))
    }
}

# Consolidate counts of common collocations appearing in different samples
nGramsSummarized <- nGrams %>%
    group_by(nGramSize, feature) %>%
    summarize(count = sum(count)) %>%
    arrange(nGramSize, desc(count))

write.csv(nGramsSummarized,
          file = 'word-prediction-dataset.csv',
          row.names = FALSE)

totalElapsedTime <- (proc.time() - scriptStartTime)[elapsedIdx]

print(paste("Total elapsed time:", totalElapsedTime))
