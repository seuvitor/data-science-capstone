library(textclean)
library(dplyr)
library(quanteda)

# Function to download the dataset if it is not present on the directory.
downloadDataset <- function() {
    datasetFileName <- "Coursera-SwiftKey.zip"
    
    datasetUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    
    if (!file.exists(datasetFileName)) {
        download.file(datasetUrl, datasetFileName)
    }
    
    if (!file.exists("final")) {
        unzip(datasetFileName)
    }
}

# Function to use when sampling lines from a text file. It uses a closure to
# remember what lines were already sampled and should be skipped when sampling
# again without replacement.
skippedLines <- function() {
    skipLine <- logical(2500000)
    lineNum <- 0
    resetLineCount <- function() {
        lineNum <<- 0
    }    
    nextLine <- function() {
        lineNum <<- lineNum + 1
    }
    setSkip <- function(skip) {
        skipLine[lineNum] <<- skip
    }
    getSkip <- function() {
        skipLine[lineNum]
    }
    getLineNum <- function() {
        lineNum
    }
    getSkipLine <- function() {
        skipLine
    }
    list(resetLineCount=resetLineCount,
         nextLine=nextLine,
         setSkip=setSkip,
         getSkip=getSkip,
         getLineNum=getLineNum,
         getSkipLine=getSkipLine)
}

# Function to sample from a text file of the SwiftKey raw text dataset.
loadCorpusSample <- function(language, category, samplingProbability) {
    skipped$resetLineCount()
    filePath <- file.path("final",
                          language,
                          paste(language, category, "txt", sep = "."))
    
    sampledLines <- character()
    con <- file(filePath, "r")
    while (TRUE) {
        line <- readLines(con, 1, skipNul = TRUE)
        if (length(line) == 0) {
            break
        }
        skipped$nextLine()
        if (!skipped$getSkip() && rbinom(1, 1, samplingProbability)) {
            sampledLines <- c(sampledLines, line)
            skipped$setSkip(TRUE)
        }
    }
    close(con)
    sampledLines
}

# Very basic text normalization to apply to the raw data.
normalizeText <- function (lines) {
    lines <- sapply(lines, replace_non_ascii)
    lines <- sapply(lines, strip)
    lines
}

# Gets a list of words to use as reference when discarding profanity from text.
getBadWords <- function() {
    badWordsFileName <- "badwords-en.txt"
    badWordsUrl <- "https://raw.githubusercontent.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/master/en"
    if (!file.exists(badWordsFileName)) {
        download.file(badWordsUrl, badWordsFileName)
    }
    readLines(badWordsFileName)
}

# Keeps the lines of text that do not contain profanity.
discardLinesWithProfanity <- function (lines) {
    badWords <- getBadWords()
    lines[!grepl(paste("\\b(", paste(badWords, collapse = "|"), ")\\b"),
                 lines,
                 ignore.case = TRUE)]
}

# Discard lines of text that appear to be automatically generated by bots on
# twitter.
discardLinesWithAutomaticMessage <- function (lines) {
    automatic <- c("thanks for the follow",
                   "thank you for the follow",
                   "thanks for the ff",
                   "thanks for the rt",
                   "thank you for the rt",
                   "thanks for the mention",
                   "thanks for the shout out")
    lines[!grepl(paste(automatic, collapse = "|"),
                 lines,
                 ignore.case = TRUE)]
}

# Function to return the minimum count of collocations of words, of each size,
# that should be considered for this dataset. These thresholds were defined
# after analysing the relative frequencies of collocations of words of size
# 2 to 5 on a 5% sample of each set of texts (twitter, blogs and news). On these
# samples, such thresholds were appropriate to keep approximately the top 10%
# most frequently ocurring collocations.
getMinCountForNGramSize <- function(size) {
    if (size == 2) {
        12
    } else if (size == 3) {
        7
    } else if (size == 4) {
        4
    } else if (size == 5) {
        4
    } else {
        0
    }
}

# Function to get the most frequently occuring words and collocations of words
# (approximately the top 10% of each class) with the respective counts of
# occurrences on the provided sample.
getTop10PercentNGramsWithCounts <- function (tokens, cardinality) {
    if (cardinality == 1) {
        dfmat <- dfm(tokens)
        freq <- textstat_frequency(dfmat)
        freq %>%
            head(n = (nrow(freq) %/% 10)) %>%
            transmute(feature, count = frequency)
    } else {
        collocations <- textstat_collocations(tokens,
                                              min_count = getMinCountForNGramSize(cardinality),
                                              size = cardinality)
        collocations %>%
            arrange(desc(count)) %>%
            transmute(feature = collocation, count)
    }
}

scriptStartTime <- proc.time()

set.seed(54321)

nGrams <- data.frame(source = factor(),
                     nGramSize = integer(),
                     feature = character(),
                     count = integer())

numBatches <- 20
elapsedIdx <- 3

downloadDataset()

for (source in c("twitter", "news", "blogs")) {
    skipped <- skippedLines()
    
    for (batch in seq(1, numBatches)) {
        readingStartTime <- proc.time()
        
        batchLines <- loadCorpusSample("en_US",
                                       source,
                                       samplingProbability = 1 /
                                           (numBatches - batch + 1)) %>%
            normalizeText()
        batchTokens <- batchLines %>%
            discardLinesWithAutomaticMessage() %>%
            discardLinesWithProfanity() %>%
            corpus() %>%
            tokens(remove_punct = TRUE)
        
        readingElapsedTime <- (proc.time() - readingStartTime)[elapsedIdx]
        linesRead <- length(batchLines)
        wordsRead <- sum(sapply(strsplit(batchLines, " "), length))
        print(paste0("Finished reading and cleaning batch ", batch, "/",
                     numBatches, " of ", source, " texts. Lines read: ",
                     linesRead, ". Words read: ", wordsRead, ". Elapsed time: ",
                     readingElapsedTime))

        for (size in seq(1, 5)) {
            statsStartTime <- proc.time()
            nGrams <- rbind(nGrams,
                            cbind(source = as.factor(source),
                                  nGramSize = size,
                                  getTop10PercentNGramsWithCounts(batchTokens,
                                                                  size)))
            statsElapsedTime <- (proc.time() - statsStartTime)[elapsedIdx]
            print(paste0("Finished processing ", size, "-grams on batch ",
                         batch, "/", numBatches, " of ", source,
                         " texts. Elapsed time: ", statsElapsedTime))
        }
    }
}

# Consolidate counts of common collocations appearing in different samples
nGramsSummarized <- nGrams %>%
    group_by(source, nGramSize, feature) %>%
    summarize(count = sum(count)) %>%
    arrange(source, nGramSize, desc(count))

write.csv(nGramsSummarized,
          file = 'word-prediction-dataset.csv',
          row.names = FALSE)

totalElapsedTime <- (proc.time() - scriptStartTime)[elapsedIdx]

print(paste("Total elapsed time:", totalElapsedTime))
