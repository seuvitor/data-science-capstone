---
title: "Data Science Capstone Milestone Report:"
subtitle: "Exploratory Analysis of the SwiftKey data"
author: "Vitor Dantas"
date: "10/7/2019"
output: html_document
---

```{r echo=FALSE,message=FALSE}
# Load required libraries for this study
library(dplyr)
library(ggplot2)
library(pander)

# Set the seed to enable reproducibility of the reported results
set.seed(12345)
```


## Executive summary

This report intends to present information about the raw data provided by SwiftKey for the Coursera
Johns Hopkins Data Science Capstone, as well as a new tidy dataset that we prepared based on that
raw data, that we intend to use for further analysis and as input to machine learning methods when
we build a product to predict the next word based on the previous words typed by a user.


## Exploratory data analysis


### The raw text data by SwiftKey

The raw data provided by SwiftKey for this study is available on the following location:

https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip

It comprises unstructured text from tweets, blog posts and news articles stored in several files,
three of them are in english: "en_US.blogs.txt", "en_US.news.txt" and "en_US.twitter.txt".

While exploring the data, we recorded the number of lines and words on each source. We can see
that although the number of lines of text is very different on each source, the total number of
words is similar. We might use this knowledge later to mix the data from the three sources without
too much fear of one type of language used in one source overweighting the others.

Here we show those basic statistics of the files and the time it took to read, clean and analyze the
data. We will discuss this processing later.

| File              | Lines     | Words      | Time reading/cleaning (minutes) | Time analyzing (minutes) |
|-------------------|-----------|------------|---------------------------------|--------------------------|
| en_US.twitter.txt | 2,360,148 | 29,371,063 | 88.74                           | 95.16                    |
| en_US.news.txt    | 1,010,242 | 33,476,255 | 43.50                           | 146.66                   |
| en_US.blogs.txt   | 899,288   | 36,826,702 | 41.19                           | 190.06                   |


### Basic data cleaning

After some exploration, we identified the need to do some basic cleaning on the data before
deepening the analysis.

First, we use the [textclean](https://cran.r-project.org/web/packages/textclean/index.html) library
to normalize the text to ASCII characters and we also remove symbols and extra blank space.

Next we discard the texts that might contain profanity (because we do not want to suggest that to
our users). For that end we use the list of bad words from this repository:
https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/

After that, we noticed a very high frequency of some messages on twitter such as "thanks for the
follow" or "thanks for the rt" that appear to be automatically generated by bots, so we discard
those too because we want to predict language as being typed by a human being.


### Considerations on processing the data

For the task of predicting the next word to be typed by a user, we assume that it will be useful to
estimate the probability of occurrence of words and of collocations of words in the english
language, and for that end, and given the resources currently available, we will assume that the
texts in the SwiftKey raw data are a representative sample of the population.

We would then like to make use of the [quanteda R library](https://quanteda.io/) to calculate the
frequencies of words and collocations on these corpora of text.

Given the large size of those files and the limited computational resources we have, we could not
compute the desired statistics at once. Instead, after some attempts, we settled with splitting each
file in 20 batches to be analysed separately and then combining the results.

Moreover, the time required to compute the frequencies of collocations without specifying minimum
counts was prohibitive, so we decided to define such limits after carefully analysing a random
sample of 5% of the lines of each file to estimate what minimum count would be appropriate to
retain at least the top 10% most ocurring collocations of each collocation size. The resulting
thresholds where:

* 12 occurrences for 2-grams,
* 7 ocurrences for 3-grams,
* 4 ocurrences for 4-grams and
* 4 ocurrences for 5-grams.

If those restrictions prove later to be damaging to the predictive power of our product, we will
revisit them.

There is certainly some loss of information when using this approach, because there may be hidden
patterns in the data so that a particular n-gram that globally occurs very frequently might not rise
to the top 10% on any of the 20 samples of the file. However, we reduced that risk by taking random
samples on each pass instead of just splitting the file and reading it sequentially.

Even under such restrictions, it takes a long time to count and rank the n-grams using the quanteda
library. The total time required to read and process all the files was **over 600 minutes**. Earlier
we showed the total time to process each source, and now we break down the time to analyze n-grams
by the size of the ngram on each source.

| Time to analyze (minutes) | twitter | news   | blogs  |
|---------------------------|---------|--------|--------|
| 1-grams                   | 0.59    | 0.48   | 0.45   |
| 2-grams                   | 26.27   | 44.42  | 46.41  |
| 3-grams                   | 37.90   | 58.66  | 80.96  |
| 4-grams                   | 25.14   | 36.45  | 53.26  |
| 5-grams                   | 5.26    | 6.66   | 8.98   |
| Total                     | 95.16   | 146.66 | 190.06 |


## Preliminary results


### The word prediction dataset

After combining the statistics previously described, we saved them in a tidy dataset with four
variables:

- **source**, a factor variable with possible values "twitter", "blogs" and "news"
- **nGramSize**, which is 1 for single words, 2 for 2-grams etc.
- **feature**, the single word or collocation of words
- **count**, the number of times the word or collocation appears

When loading this dataset, you might want to specify the column types so that the source column is
loaded as a factor variable while the feature column is loaded as a character variable.

```{r}
nGrams <- read.csv(file = 'word-prediction-dataset.csv',
                   colClasses = c("factor", "integer", "character", "integer"))
```

We are going to use this new dataset for further analysis and for the development of our word
prediction product. After the removal of part of the data in the data cleaning, and the removal of
the unfrequent words and collocations of words by the application of the thresholds already
discussed, our dataset contains `r nrow(nGrams)` observations of the four variables just presented.
Here we display a random sample of 10 observations to see what the dataset looks like.

```{r echo=FALSE}
pander(nGrams[sort(head(sample(nrow(nGrams)), 10)), ])
```


### Frequency of words and collocations of words on the dataset

We show now which single words and collocations of 2 to 5 words appeared more frequently on the
dataset.

```{r echo=FALSE}
topNGrams <- nGrams %>%
    group_by(nGramSize, feature) %>%
    summarize(count = sum(count)) %>%
    top_n(n = 10, wt = count) %>%
    arrange(desc(count))
```

```{r echo=FALSE,fig.width=6,fig.height=3}
ggplot(topNGrams[topNGrams$nGramSize == 1, ], aes(x = reorder(feature, count), y = count/1000)) +
    geom_bar(stat = 'identity') +
    coord_flip() +
    labs(title = "Most frequent single words",
         x = "",
         y = "count (thousands)")
```

```{r echo=FALSE,fig.width=6,fig.height=3}
ggplot(topNGrams[topNGrams$nGramSize == 2, ], aes(x = reorder(feature, count), y = count/1000)) +
    geom_bar(stat = 'identity') +
    coord_flip() +
    labs(title = "Most frequent 2-grams",
         x = "",
         y = "count (thousands)")
```

```{r echo=FALSE,fig.width=6,fig.height=3}
ggplot(topNGrams[topNGrams$nGramSize == 3, ], aes(x = reorder(feature, count), y = count/1000)) +
    geom_bar(stat = 'identity') +
    coord_flip() +
    labs(title = "Most frequent 3-grams",
         x = "",
         y = "count (thousands)")
```

```{r echo=FALSE,fig.width=6,fig.height=3}
ggplot(topNGrams[topNGrams$nGramSize == 4, ], aes(x = reorder(feature, count), y = count/1000)) +
    geom_bar(stat = 'identity') +
    coord_flip() +
    labs(title = "Most frequent 4-grams",
         x = "",
         y = "count (thousands)")
```

```{r echo=FALSE,fig.width=6,fig.height=3}
ggplot(topNGrams[topNGrams$nGramSize == 5, ], aes(x = reorder(feature, count), y = count/1000)) +
    geom_bar(stat = 'identity') +
    coord_flip() +
    labs(title = "Most frequent 5-grams",
         x = "",
         y = "count (thousands)")
```


## Further work

We intend to use the dataset that we built here, as well as the knowledge we obtained on this
exploratory analysis, to build a predictive model for predicting the next word a user will type
on our application, based on the previous words that he typed and the probability that specific
words will follow such words, under our probabilistic model.
